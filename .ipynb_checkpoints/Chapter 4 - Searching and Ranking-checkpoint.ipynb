{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html lang=\"mul\" class=\"no-js\">\\n<h'\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "#from pysqlite3 import dbapi2 as sqlite\n",
    "from sqlite3 import dbapi2 as sqlite\n",
    "# urllib.parse\n",
    "\n",
    "\n",
    "\n",
    "class crawler:\n",
    "    #Initialize the crawler with the name of the databse\n",
    "    def __init__(self,dbname):\n",
    "        self.con=sqlite.connect(dbname)\n",
    "    def __del__(self):\n",
    "        self.con.close( )\n",
    "    def dbcommit(self):\n",
    "        self.con.commit( )\n",
    "    #Auxiliary function for getting and entry id and adding it if it is not present\n",
    "    def getentryid(self, table, field, value, createnew=True):\n",
    "        return None\n",
    "    #Index individual page\n",
    "    def addtoindex(self,url,soup):\n",
    "        print (\"Indexing %s\" %url)\n",
    "        return None\n",
    "    #Extract the text from HTML page withou tags\n",
    "    def gettextonly(self,soup):\n",
    "        v=soup.string\n",
    "        if v==None:\n",
    "            c=soup.contents\n",
    "            resulttext=''\n",
    "            for t in c:\n",
    "                subtext=self.gettextonly(t)\n",
    "                resulttext+=subtext+'\\n'\n",
    "            return resulttext\n",
    "        else:\n",
    "            return v.strip()\n",
    "\n",
    "    #Separate the words by any non-whitespace character\n",
    "    def separatewords (self, text):\n",
    "        splitter=re.compile('\\\\W*')\n",
    "        return [s.lower() for s in splitter.split(text) if s!='']\n",
    "    \n",
    "    #Check if url is already indexed\n",
    "    def isindexed(self, url):\n",
    "        return False\n",
    "    #Add link between two pages\n",
    "    def addlinkref(self, urlFrom, urlTo, linkText):\n",
    "        pass\n",
    "    #Do a breath first search to given depth\n",
    "    def crawl(self, pages, depth=2):\n",
    "        \n",
    "        for i in range(depth):\n",
    "            newpages = set()\n",
    "            for page in pages:\n",
    "                try:\n",
    "                    c=urllib.request.urlopen(page)\n",
    "                except:\n",
    "                    print (\"Could not open %s\" %page)\n",
    "                    continue\n",
    "                soup = BeautifulSoup(c.read(), 'lxml')\n",
    "                self.addtoindex(page,soup)\n",
    "                \n",
    "                links = soup(\"a\")\n",
    "                for link in links:\n",
    "                    if ('href' in dict(link.attrs)):\n",
    "                        url = urllib.request.urljoin(page, link['href'])\n",
    "                        if url.find(\"'\")!= -1: \n",
    "                            continue\n",
    "                        url=url.split('#')[0]  # remove location portion\n",
    "                        if url[0:4]=='http' and not self.isindexed(url):\n",
    "                            newpages.add(url)\n",
    "                        linkText=self.gettextonly(link)\n",
    "                        self.addlinkref(page,url,linkText)\n",
    "                self.dbcommit()\n",
    "            pages=newpages\n",
    "        \n",
    "        pass\n",
    "    def createindextables(self):\n",
    "        self.con.execute('create table urllist(url)')\n",
    "        self.con.execute('create table wordlist(word)')\n",
    "        self.con.execute('create table wordlocation(urlid,wordid,location)')\n",
    "        self.con.execute('create table link(fromid integer,toid integer)')\n",
    "        self.con.execute('create table linkwords(wordid,linkid)')\n",
    "        self.con.execute('create index wordidx on wordlist(word)')\n",
    "        self.con.execute('create index urlidx on urllist(url)')\n",
    "        self.con.execute('create index wordurlidx on wordlocation(wordid)')\n",
    "        self.con.execute('create index urltoidx on link(toid)') \n",
    "        self.con.execute('create index urlfromidx on link(fromid)') \n",
    "        self.dbcommit( )\n",
    "    \n",
    "    \n",
    "#wp = urllib.request.urlopen(\"http://google.com\")\n",
    "#pw = wp.read()\n",
    "#print(pw)\n",
    "\n",
    "c=urllib.request.urlopen('https://www.wikipedia.org/')\n",
    "contents=c.read()\n",
    "print (contents[0:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ignorewords=set(['the','of','to','and','a','in','is','it'])\n",
    "\n",
    "\n",
    "pagelist=['http://wikipedia.com']\n",
    "crawlers = crawler('searchindex.db')\n",
    "crawlers.createindextables()\n",
    "#crawlers.crawl(pagelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
